# rag_langchain.py
import os
from pathlib import Path
from typing import List, Optional
from collections import defaultdict, deque
import json
import logging

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import BaseMessage
from langchain_chroma import Chroma

# è‡ªå®šä¹‰æ¨¡å—ï¼ˆç¡®ä¿è¿™äº›æ–‡ä»¶å­˜åœ¨ï¼‰
from embedding_client import get_embeddings
from qwen_client import call_qwen

# DuckDuckGo æœç´¢
from duckduckgo_search import DDGS

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€ä¼šè¯å†å²ï¼ˆè½»é‡çº§ demo ç”¨ï¼‰
SESSION_HISTORY = defaultdict(lambda: deque(maxlen=2))


# ==============================
# 1. è‡ªå®šä¹‰ Embedding Function
# ==============================
class CustomEmbeddingFunction:
    def __call__(self, input: List[str]) -> List[List[float]]:
        embeddings = get_embeddings(input)
        return embeddings.tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self(texts)

    def embed_query(self, text: str) -> List[float]:
        emb = self([text])
        vec = emb[0]
        if hasattr(vec, 'tolist'):
            return vec.tolist()
        return list(vec)


# ==============================
# 2. è‡ªå®šä¹‰ LLMï¼ˆQwenï¼‰
# ==============================
class CustomQwenLLM:
    def invoke(self, input: str | BaseMessage) -> str:
        if hasattr(input, 'content'):
            prompt = input.content
        else:
            prompt = str(input)
        return call_qwen(prompt, model="qwen-max")

    def __call__(self, input: str | BaseMessage, config=None) -> str:
        return self.invoke(input)


# ==============================
# 3. å¯ä¿¡æœç´¢å‡½æ•°ï¼ˆDuckDuckGoï¼‰
# ==============================
def trusted_search(query: str, num_results: int = 3) -> str:
    """ä½¿ç”¨ DuckDuckGo å…è´¹æœç´¢è·å–æ‘˜è¦"""
    try:
        with DDGS() as ddgs:
            results = ddgs.text(
                keywords=query,
                region="zh-cn",
                safesearch="moderate",
                max_results=num_results
            )
            snippets = [r["body"] for r in results if r.get("body")]
            return "\n".join(snippets[:num_results])
    except Exception as e:
        logger.warning(f"ğŸ” DuckDuckGo æœç´¢å¤±è´¥: {e}")
        return ""


# ==============================
# 4. ä¸» RAG ç±»
# ==============================
class LangChainRAG:
    def __init__(
        self,
        document_path: str,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        base_persist_dir: str = "./chroma_db_lc"
    ):
        self.document_path = Path(document_path)
        use_qwen = os.getenv("USE_QWEN_EMBEDDING", "false").lower() == "true"
        embed_suffix = "qwen_1536" if use_qwen else "minilm_384"
        doc_name = self.document_path.stem
        persist_directory = Path(base_persist_dir) / doc_name / embed_suffix
        persist_directory.mkdir(parents=True, exist_ok=True)

        documents = self._load_documents()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        splits = text_splitter.split_documents(documents)

        embedding_func = CustomEmbeddingFunction()
        self.vectorstore = Chroma(
            collection_name="rag_docs",
            embedding_function=embedding_func,
            persist_directory=str(persist_directory),
            collection_metadata={"hnsw:space": "cosine"}
        )

        if self.vectorstore._collection.count() == 0:
            logger.info(f"ğŸ”„ é¦–æ¬¡æ„å»ºå‘é‡åº“ï¼Œå…± {len(splits)} ä¸ªæ–‡æœ¬å—")
            self.vectorstore.add_documents(splits)
        else:
            logger.info(f"ğŸ“‚ åŠ è½½å·²æœ‰å‘é‡åº“ï¼Œå…± {self.vectorstore._collection.count()} ä¸ªæ–‡æœ¬å—")

        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})
        self.qa_chain = self._build_rag_chain()

    def _load_documents(self) -> List[Document]:
        with open(self.document_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        docs = []
        if "pages" in data:
            for page in data["pages"]:
                content = page["content"]
                source = f"{data['metadata']['source_file']}:p{page['page_number']}"
                docs.append(Document(page_content=content, metadata={"source": source}))
        else:
            texts = data if isinstance(data, list) else [data.get("text", "")]
            for text in texts:
                docs.append(Document(page_content=text, metadata={"source": "unknown"}))
        return docs

    def _build_rag_chain(self):
        prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ AI åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ã€ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ã€‚
- å¦‚æœä¸Šä¸‹æ–‡åŒ…å«è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·ç›´æ¥ç»™å‡º**ç®€æ´ã€å‡†ç¡®**çš„ç­”æ¡ˆã€‚
- å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®šâ€ã€‚
- ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œä¸è¦è§£é‡Šæ¨ç†è¿‡ç¨‹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š
{question}

ã€å›ç­”ã€‘
"""
        prompt = ChatPromptTemplate.from_template(prompt_template)
        llm = CustomQwenLLM()

        def format_docs(docs):
            total_len = 0
            selected = []
            max_chars = 2000
            for d in docs:
                if total_len + len(d.page_content) > max_chars:
                    break
                selected.append(d.page_content)
                total_len += len(d.page_content)
            return "\n\n".join(selected)

        rag_chain = (
            {"context": self.retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
        )
        return rag_chain

    def ask(self, question: str) -> str:
        return self.qa_chain.invoke(question)


# ==============================
# 5. å¸¦è®°å¿† + æœç´¢å…œåº•çš„ RAG
# ==============================
class LangChainRAGWithMemory(LangChainRAG):
    def ask(self, question: str, session_id: Optional[str] = None) -> str:
        # Step 1: æ„å»ºå®é™…æé—®å†…å®¹ï¼ˆå¸¦æˆ–ä¸å¸¦å†å²ï¼‰
        if session_id is None:
            actual_question = question
            is_with_memory = False
        else:
            history = SESSION_HISTORY[session_id]
            if history:
                history_text = "\n".join([
                    f"ç”¨æˆ·ä¹‹å‰é—®ï¼š{q}\nåŠ©æ‰‹å›ç­”ï¼š{a}"
                    for q, a in history
                ])
                actual_question = (
                    f"ã€å¯¹è¯å†å²ã€‘\n{history_text}\n\n"
                    f"ã€å½“å‰é—®é¢˜ã€‘\n{question}"
                )
            else:
                actual_question = question
            is_with_memory = True

        # Step 2: å…ˆèµ°æœ¬åœ° RAG
        answer = super().ask(actual_question)

        # Step 3: å¦‚æœæ— ç­”æ¡ˆï¼Œè§¦å‘æœç´¢
        if "æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®š" in answer:
            logger.info("ğŸ” æœ¬åœ°æ— ç­”æ¡ˆï¼Œè§¦å‘ DuckDuckGo æœç´¢...")
            search_results = trusted_search(question)  # ç”¨åŸå§‹ question æœç´¢
            if search_results.strip():
                fallback_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ AI åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ã€ç½‘ç»œæœç´¢ç»“æœã€‘å›ç­”é—®é¢˜ã€‚
- åªä½¿ç”¨æœç´¢ç»“æœä¸­çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ã€‚
- å¦‚æœç»“æœä¸ç›¸å…³æˆ–ä¸ºç©ºï¼Œè¯·å›ç­”â€œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ã€‚

ã€æœç´¢ç»“æœã€‘
{search_results}

ã€é—®é¢˜ã€‘
{question}

ã€å›ç­”ã€‘
"""
                llm = CustomQwenLLM()
                answer = llm.invoke(fallback_prompt)
            else:
                answer = "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        # Step 4: ä¿å­˜åˆ°ä¼šè¯å†å²ï¼ˆä»…å½“ session_id æä¾›æ—¶ï¼‰
        if session_id is not None:
            SESSION_HISTORY[session_id].append((question, answer))

        return answer


