# llm_client.py
"""
ç»Ÿä¸€è°ƒç”¨å¤§æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ Qwen + Qianfan OpenAI å…¼å®¹æ¨¡å¼ï¼‰
ä½œè€…ï¼šAI å¼€å‘è€…
æ—¥æœŸï¼š2026å¹´2æœˆ
"""

import os
import time
from dotenv import load_dotenv
from openai import OpenAI, APIError, Timeout, RateLimitError

load_dotenv()

# ==============================
# 1. é€šä¹‰åƒé—® (Qwen) - åŸç”Ÿ API
# ==============================
def call_qwen(prompt: str, model="qwen-max") -> str:
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        return "âŒ ç¼ºå°‘ DASHSCOPE_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶"

    client = OpenAI(
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        api_key=api_key,
        timeout=30
    )
    
    for i in range(3):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except (APIError, Timeout, RateLimitError) as e:
            err_msg = str(e)
            print(f"  â†’ Qwen å°è¯• {i+1} å¤±è´¥: {err_msg[:100]}")
            if i < 2:
                time.sleep(2)
        except Exception as e:
            return f"âŒ Qwen è°ƒç”¨å¼‚å¸¸: {e}"
    
    return "âŒ Qwen æ‰€æœ‰é‡è¯•å¤±è´¥"


# ==============================
# 2. ç™¾åº¦åƒå¸† (Qianfan) - OpenAI å…¼å®¹æ¨¡å¼
# ==============================
def call_qianfan(prompt: str, model="ernie-3.5-8k") -> str:
    api_key = os.getenv("QIANFAN_OPENAI_API_KEY")
    if not api_key:
        return "âŒ ç¼ºå°‘ QIANFAN_OPENAI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶"

    # æ¨¡å‹åç§°æ˜ å°„ï¼ˆç¡®ä¿ä½¿ç”¨å®˜æ–¹æ”¯æŒçš„åç§°ï¼‰
    valid_models = {
        "ernie-3.5-8k",
        "ernie-speed-8k",
        "ernie-4.0-8k",
        "ernie-4.5-turbo-128k",
    }
    if model not in valid_models:
        return f"âŒ ä¸æ”¯æŒçš„ç™¾åº¦æ¨¡å‹: '{model}'ã€‚è¯·é€‰æ‹©: {sorted(valid_models)}"

    client = OpenAI(
        base_url="https://qianfan.baidubce.com/v2",
        api_key=api_key,
        timeout=30
    )
    
    for i in range(3):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                top_p=0.9
            )
            return response.choices[0].message.content
            
        except (APIError, Timeout, RateLimitError) as e:
            err_msg = str(e)
            print(f"  â†’ Qianfan å°è¯• {i+1} å¤±è´¥: {err_msg[:100]}")
            if i < 2:
                time.sleep(2)
        except Exception as e:
            return f"âŒ Qianfan è°ƒç”¨å¼‚å¸¸: {e}"
    
    return "âŒ Qianfan æ‰€æœ‰é‡è¯•å¤±è´¥"


# ==============================
# 3. ç»Ÿä¸€å…¥å£å‡½æ•°
# ==============================
def call_llm(provider: str, prompt: str, model: str = None) -> str:
    """
    ç»Ÿä¸€è°ƒç”¨å¤§æ¨¡å‹
    Args:
        provider: 'qwen' æˆ– 'qianfan'
        prompt: ç”¨æˆ·è¾“å…¥
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
    """
    print(f"ğŸš€ è°ƒç”¨ {provider.upper()} ...")
    
    providers = {
        "qwen": (call_qwen, "qwen-max"),
        "qianfan": (call_qianfan, "ernie-3.5-8k"),
    }

    if provider not in providers:
        return f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}ï¼ˆæ”¯æŒ: qwen, qianfanï¼‰"

    func, default_model = providers[provider]
    actual_model = model or default_model
    
    if actual_model != default_model:
        print(f"   â†’ ä½¿ç”¨æ¨¡å‹: {actual_model}")
    
    return func(prompt, actual_model)