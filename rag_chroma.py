# rag_chroma.py
import os
import json
import logging
import re
from pathlib import Path
from typing import List, Dict, Any, Optional

import chromadb

# è‡ªå®šä¹‰æ¨¡å—ï¼ˆå¤ç”¨ Day 10 çš„é€»è¾‘ï¼‰
from embedding_client import get_embeddings  # æ”¯æŒ MiniLM / Qwen
from qwen_client import call_qwen

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGWithChroma:
    def __init__(
        self,
        document_path: str,
        chunk_size: int = 500,
        base_persist_dir: Optional[str] = "./chroma_db"
    ):
        """
        ä½¿ç”¨ Chroma ä½œä¸ºå‘é‡æ•°æ®åº“çš„ RAG ç³»ç»Ÿ
        
        Args:
            document_path: JSON æ–‡æ¡£è·¯å¾„
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            base_persist_dir: åŸºç¡€æŒä¹…åŒ–ç›®å½•ï¼ˆä¼šè‡ªåŠ¨è¿½åŠ  embedding ç±»å‹å­ç›®å½•ï¼‰
        """
        self.document_path = Path(document_path)
        self.chunk_size = chunk_size
        
        # ğŸ”¥ è‡ªåŠ¨æ ¹æ® embedding æ¨¡å¼é€‰æ‹©å­ç›®å½•
        use_qwen = os.getenv("USE_QWEN_EMBEDDING", "false").lower() == "true"
        embed_suffix = "qwen_1536" if use_qwen else "minilm_384"
        
        # æ„å»ºå¸¦æ–‡æ¡£åçš„è·¯å¾„ï¼Œé¿å…ä¸åŒæ–‡æ¡£æ··ç”¨
        doc_name = self.document_path.stem  # e.g., "Docker"
        persist_directory = Path(base_persist_dir) / doc_name / embed_suffix if base_persist_dir else None

        if persist_directory:
            persist_directory.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ ä½¿ç”¨æŒä¹…åŒ–ç›®å½•: {persist_directory}")
            self.client = chromadb.PersistentClient(path=str(persist_directory))
        else:
            logger.info("ğŸ§  ä½¿ç”¨å†…å­˜æ¨¡å¼ï¼ˆä¸æŒä¹…åŒ–ï¼‰")
            self.client = chromadb.EphemeralClient()

        # åˆ›å»ºé›†åˆï¼ˆè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨ï¼‰
        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        )
        
        # åŠ è½½å¹¶æ„å»ºæ•°æ®åº“
        self._load_documents()
        self._chunk_documents()
        self._build_chroma_index()

    def _load_documents(self):
        if not self.document_path.exists():
            raise FileNotFoundError(f"æ–‡æ¡£ä¸å­˜åœ¨: {self.document_path}")
        
        with open(self.document_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if "pages" in data:
            texts = [page["content"] for page in data["pages"]]
            sources = [f"{data['metadata']['source_file']}:p{page['page_number']}" 
                      for page in data["pages"]]
        else:
            texts = data if isinstance(data, list) else [data.get("text", "")]
            sources = ["unknown"] * len(texts)
        
        self.raw_texts = texts
        self.sources = sources
        logger.info(f"âœ… åŠ è½½ {len(texts)} æ®µåŸå§‹æ–‡æœ¬")

    def _split_into_sentences(self, text: str) -> List[str]:
        sentence_endings = r'[ã€‚ï¼ï¼Ÿ\.!?]'
        parts = re.split(f'({sentence_endings})', text)
        sentences = []
        i = 0
        while i < len(parts):
            if i + 1 < len(parts) and re.match(sentence_endings, parts[i + 1]):
                sentences.append(parts[i] + parts[i + 1])
                i += 2
            else:
                if parts[i].strip():
                    sentences.append(parts[i])
                i += 1
        return [s.strip() for s in sentences if s.strip()]

    def _chunk_documents(self):
        self.chunks = []
        self.metadatas = []
        self.ids = []

        chunk_id = 0
        for i, text in enumerate(self.raw_texts):
            if not text.strip():
                continue

            source = self.sources[i]
            current_chunk = ""
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            
            for para in paragraphs:
                if len(para) <= self.chunk_size:
                    if current_chunk and len(current_chunk) + len(para) + 2 <= self.chunk_size:
                        current_chunk += "\n\n" + para
                    else:
                        if current_chunk:
                            self.chunks.append(current_chunk)
                            self.metadatas.append({"source": source})
                            self.ids.append(f"chunk_{chunk_id}")
                            chunk_id += 1
                        current_chunk = para
                else:
                    sentences = self._split_into_sentences(para)
                    for sent in sentences:
                        if current_chunk and len(current_chunk) + len(sent) + 1 <= self.chunk_size:
                            current_chunk += " " + sent
                        else:
                            if current_chunk:
                                self.chunks.append(current_chunk)
                                self.metadatas.append({"source": source})
                                self.ids.append(f"chunk_{chunk_id}")
                                chunk_id += 1
                            current_chunk = sent
            
            if current_chunk:
                self.chunks.append(current_chunk)
                self.metadatas.append({"source": source})
                self.ids.append(f"chunk_{chunk_id}")
                chunk_id += 1

        logger.info(f"âœ… åˆ‡åˆ†ä¸º {len(self.chunks)} ä¸ªè¯­ä¹‰æ–‡æœ¬å—")

    def _build_chroma_index(self):
        """å°†æ–‡æœ¬å— + å‘é‡ + å…ƒæ•°æ®å­˜å…¥ Chroma"""
        if not self.chunks:
            logger.warning("âš ï¸ æ— æ–‡æœ¬å—ï¼Œè·³è¿‡ç´¢å¼•æ„å»º")
            return

        # ğŸŒŸ å…³é”®ï¼šä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„ embedding å‡½æ•°ï¼ˆMiniLM æˆ– Qwenï¼‰
        logger.info("æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶å­˜å…¥ Chroma...")
        embeddings = get_embeddings(self.chunks).tolist()  # Chroma éœ€è¦ list[float]

        # æ‰¹é‡æ·»åŠ ï¼ˆChroma æ”¯æŒ up to 4168 æ¡/æ¬¡ï¼‰
        batch_size = 1000
        for i in range(0, len(self.chunks), batch_size):
            self.collection.add(
                ids=self.ids[i:i+batch_size],
                embeddings=embeddings[i:i+batch_size],
                documents=self.chunks[i:i+batch_size],
                metadatas=self.metadatas[i:i+batch_size]
            )

        logger.info(f"âœ… Chroma æ•°æ®åº“æ„å»ºå®Œæˆ | æ–‡æœ¬å—æ•°é‡: {len(self.chunks)}")

    def retrieve(
        self,
        query: str,
        k: int = 3,
        where: Optional[Dict[str, str]] = None
    ) -> List[Dict[str, Any]]:
        """
        ä» Chroma æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬
        
        Args:
            query: æŸ¥è¯¢é—®é¢˜
            k: è¿”å› top-k ç»“æœ
            where: å…ƒæ•°æ®è¿‡æ»¤ï¼Œå¦‚ {"source": "xxx.pdf:p5"}
        """
        query_embedding = get_embeddings([query]).tolist()[0]
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k,
            where=where,
            include=["documents", "metadatas", "distances"]
        )
        
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        retrieved = []
        for i in range(len(results["ids"][0])):
            retrieved.append({
                "content": results["documents"][0][i],
                "metadata": results["metadatas"][0][i],
                "score": 1.0 - results["distances"][0][i]  # Chroma cosine distance â†’ similarity
            })
        return retrieved

    def generate_answer(self, question: str, context_list: List[str], max_contexts: int = 3) -> str:
        if not context_list:
            return "æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®š"

        selected = []
        total_len = 0
        max_chars = 2000
        for ctx in context_list[:max_contexts]:
            if total_len + len(ctx) > max_chars:
                break
            selected.append(ctx)
            total_len += len(ctx)

        context = "\n\n".join(selected)
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ AI åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ã€ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ã€‚
- å¦‚æœä¸Šä¸‹æ–‡åŒ…å«è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·ç›´æ¥ç»™å‡º**ç®€æ´ã€å‡†ç¡®**çš„ç­”æ¡ˆã€‚
- å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®šâ€ã€‚
- ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œä¸è¦è§£é‡Šæ¨ç†è¿‡ç¨‹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š
{question}

ã€å›ç­”ã€‘
"""
        try:
            return call_qwen(prompt, model="qwen-max").strip()
        except Exception as e:
            logger.error(f"Qwen è°ƒç”¨å¤±è´¥: {e}")
            return "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ã€‚"

    def ask(self, question: str, top_k: int = 3) -> Dict[str, Any]:
        logger.info(f"ğŸ” Chroma æ£€ç´¢ä¸­: '{question}'")
        retrieved = self.retrieve(question, k=top_k)
        contexts = [item["content"] for item in retrieved]
        answer = self.generate_answer(question, contexts)
        return {"question": question, "answer": answer, "retrieved_chunks": retrieved}


# ======================
# ä½¿ç”¨ç¤ºä¾‹
# ======================
if __name__ == "__main__":
    import os
    # åˆ‡æ¢ Embedding æ¨¡å¼
    os.environ["USE_QWEN_EMBEDDING"] = "true"  # â† æ”¹è¿™é‡Œï¼true=Qwen(1536d), false=MiniLM(384d)

    # è‡ªåŠ¨æŒ‰æ–‡æ¡£å + embedding ç±»å‹éš”ç¦»æ•°æ®åº“
    rag = RAGWithChroma(
        document_path="data/processed/Docker.json",
        chunk_size=500,
        base_persist_dir="./chroma_db"  # æœ€ç»ˆè·¯å¾„: ./chroma_db/Docker/qwen_1536/
    )
    
    questions = [
        "Dockeré•œåƒå¦‚ä½•æ„å»ºï¼Ÿ",
        "å®¹å™¨å’Œé•œåƒçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•æŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨ï¼Ÿ"
    ]
    
    for q in questions:
        print("\n" + "="*60)
        result = rag.ask(q)
        print(f"â“ é—®é¢˜: {result['question']}")
        print(f"âœ… ç­”æ¡ˆ: {result['answer']}")
        
        print("\nğŸ“š æ£€ç´¢æ¥æº:")
        for chunk in result["retrieved_chunks"]:
            print(f"  - ({chunk['metadata']['source']})")