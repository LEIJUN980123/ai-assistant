# rag_system.py
import json
import logging
import re
import numpy as np
import faiss
from pathlib import Path
from typing import List, Dict, Any

# è‡ªå®šä¹‰æ¨¡å—
from embedding_client import get_embeddings
from qwen_client import call_qwen  # ä½ å·²æœ‰çš„ Qwen è°ƒç”¨å‡½æ•°

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGSystem:
    def __init__(self, document_path: str, chunk_size: int = 500):
        self.document_path = Path(document_path)
        self.chunk_size = chunk_size
        self.chunks = []
        self.chunk_metadatas = []
        self.index = None
        
        self._load_documents()
        self._chunk_documents()
        self._build_vector_index()

    def _load_documents(self):
        if not self.document_path.exists():
            raise FileNotFoundError(f"æ–‡æ¡£ä¸å­˜åœ¨: {self.document_path}")
        
        with open(self.document_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if "pages" in data:
            texts = [page["content"] for page in data["pages"]]
            sources = [f"{data['metadata']['source_file']}:p{page['page_number']}" 
                      for page in data["pages"]]
        else:
            texts = data if isinstance(data, list) else [data.get("text", "")]
            sources = ["unknown"] * len(texts)
        
        self.raw_texts = texts
        self.sources = sources
        logger.info(f"âœ… åŠ è½½ {len(texts)} æ®µåŸå§‹æ–‡æœ¬")

    def _split_into_sentences(self, text: str) -> List[str]:
        sentence_endings = r'[ã€‚ï¼ï¼Ÿ\.!?]'
        parts = re.split(f'({sentence_endings})', text)
        sentences = []
        i = 0
        while i < len(parts):
            if i + 1 < len(parts) and re.match(sentence_endings, parts[i + 1]):
                sentences.append(parts[i] + parts[i + 1])
                i += 2
            else:
                if parts[i].strip():
                    sentences.append(parts[i])
                i += 1
        return [s.strip() for s in sentences if s.strip()]

    def _chunk_documents(self):
        self.chunks = []
        self.chunk_metadatas = []

        for i, text in enumerate(self.raw_texts):
            if not text.strip():
                continue

            source = self.sources[i]
            current_chunk = ""
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            
            for para in paragraphs:
                if len(para) <= self.chunk_size:
                    if current_chunk and len(current_chunk) + len(para) + 2 <= self.chunk_size:
                        current_chunk += "\n\n" + para
                    else:
                        if current_chunk:
                            self.chunks.append(current_chunk)
                            self.chunk_metadatas.append({"source": source})
                        current_chunk = para
                else:
                    sentences = self._split_into_sentences(para)
                    for sent in sentences:
                        if current_chunk and len(current_chunk) + len(sent) + 1 <= self.chunk_size:
                            current_chunk += " " + sent
                        else:
                            if current_chunk:
                                self.chunks.append(current_chunk)
                                self.chunk_metadatas.append({"source": source})
                            current_chunk = sent
            
            if current_chunk:
                self.chunks.append(current_chunk)
                self.chunk_metadatas.append({"source": source})

        logger.info(f"âœ… åˆ‡åˆ†ä¸º {len(self.chunks)} ä¸ªè¯­ä¹‰æ–‡æœ¬å—")

    def _build_vector_index(self):
        if not self.chunks:
            logger.warning("âš ï¸ æ— æ–‡æœ¬å—ï¼Œè·³è¿‡ç´¢å¼•æ„å»º")
            self.index = None
            return

        # ğŸŒŸ å…³é”®ï¼šè‡ªåŠ¨é€‰æ‹© embedding æ–¹å¼
        embeddings = get_embeddings(self.chunks)
        dimension = embeddings.shape[1]
        logger.info(f"âœ… ç”Ÿæˆ {len(self.chunks)} ä¸ª {dimension} ç»´å‘é‡")

        self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ï¼ˆéœ€ normalizedï¼‰
        faiss.normalize_L2(embeddings)  # ç¡®ä¿å‘é‡å½’ä¸€åŒ–
        self.index.add(embeddings)

    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        if self.index is None:
            raise RuntimeError("å‘é‡ç´¢å¼•æœªåˆå§‹åŒ–")
        
        query_emb = get_embeddings([query])
        faiss.normalize_L2(query_emb)
        distances, indices = self.index.search(query_emb, k)
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.chunks):
                results.append({
                    "content": self.chunks[idx],
                    "metadata": self.chunk_metadatas[idx],
                    "score": float(distances[0][i])  # è¶Šæ¥è¿‘ 1 è¶Šç›¸ä¼¼
                })
        return results

    def generate_answer(self, question: str, context_list: List[str], max_contexts: int = 3) -> str:
        if not context_list:
            return "æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®š"

        selected = []
        total_len = 0
        max_chars = 2000
        for ctx in context_list[:max_contexts]:
            if total_len + len(ctx) > max_chars:
                break
            selected.append(ctx)
            total_len += len(ctx)

        context = "\n\n".join(selected)
        prompt =  f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ AI åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ã€ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ã€‚
                    - å¦‚æœä¸Šä¸‹æ–‡åŒ…å«è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·ç›´æ¥ç»™å‡º**ç®€æ´ã€å‡†ç¡®**çš„ç­”æ¡ˆã€‚
                    - å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®šâ€ã€‚
                    - ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œä¸è¦è§£é‡Šæ¨ç†è¿‡ç¨‹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š
{question}

ã€å›ç­”ã€‘
"""
        try:
            return call_qwen(prompt, model="qwen-max").strip()
        except Exception as e:
            logger.error(f"Qwen è°ƒç”¨å¤±è´¥: {e}")
            return "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ã€‚"

    def ask(self, question: str, top_k: int = 3) -> Dict[str, Any]:
        logger.info(f"ğŸ” æ£€ç´¢ä¸­: '{question}'")
        retrieved = self.retrieve(question, k=top_k)
        contexts = [item["content"] for item in retrieved]
        answer = self.generate_answer(question, contexts)
        return {"question": question, "answer": answer, "retrieved_chunks": retrieved}

