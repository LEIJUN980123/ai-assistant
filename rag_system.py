# rag_system.py
import os
import json
import logging
import torch
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# å¯¼å…¥ä½ å·²æœ‰çš„ Qwen å®¢æˆ·ç«¯
from qwen_client import call_qwen

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGSystem:
    def __init__(self, document_path: str, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        åˆå§‹åŒ– RAG ç³»ç»Ÿ
        
        Args:
            document_path (str): JSON æ–‡æ¡£è·¯å¾„ï¼ˆæ¥è‡ª pdf_to_json.pyï¼‰
            chunk_size (int): æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap (int): å—ä¹‹é—´é‡å å­—ç¬¦æ•°
        """
        self.document_path = Path(document_path)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.chunks = []          # List[str]
        self.chunk_metadatas = [] # List[Dict]
        self.index = None         # FAISS index
        self.embedding_model = None
        
        self._load_documents()
        self._chunk_documents()
        self._build_vector_index()
    
    def _load_documents(self):
        """ä» JSON åŠ è½½æ–‡æ¡£å†…å®¹"""
        if not self.document_path.exists():
            raise FileNotFoundError(f"æ–‡æ¡£ä¸å­˜åœ¨: {self.document_path}")
        
        with open(self.document_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1. æ¥è‡ª pdf_to_json.py çš„ {"pages": [...]}
        # 2. ç®€å•åˆ—è¡¨ ["text1", "text2"]
        if "pages" in data:
            texts = [page["content"] for page in data["pages"]]
            sources = [f"{data['metadata']['source_file']}:p{page['page_number']}" 
                      for page in data["pages"]]
        else:
            texts = data if isinstance(data, list) else [data.get("text", "")]
            sources = ["unknown"] * len(texts)
        
        self.raw_texts = texts
        self.sources = sources
        logger.info(f"âœ… åŠ è½½ {len(texts)} æ®µåŸå§‹æ–‡æœ¬")
    
    def _chunk_documents(self):
        """å°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå°å—"""
        self.chunks = []
        self.chunk_metadatas = []
        for i, text in enumerate(self.raw_texts):
            if not text.strip():
                continue
            text_len=len(text)
            start = 0
            while start < text_len:
                end = start + self.chunk_size
                
                chunk = text[start:end]
                if chunk.strip():
                    self.chunks.append(chunk)
                    self.chunk_metadatas.append({
                        "source": self.sources[i],
                        "chunk_id": len(self.chunks) - 1,
                        "start_char": start,
                        "end_char": min(end, text_len)
                    })
                
                if end >= text_len:
                    break  # åˆ°è¾¾æœ«å°¾ï¼Œé€€å‡º
        
                # è®¡ç®—ä¸‹ä¸€å—çš„èµ·å§‹ä½ç½®
                start = end - self.chunk_overlap
        
                # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢ä¸å‰è¿›
                if start >= end:
                    start = end
                
        logger.info(f"âœ… åˆ‡åˆ†ä¸º {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
    
    def _build_vector_index(self):
        """æ„å»º FAISS å‘é‡ç´¢å¼•ï¼ˆå¸¦ç¼“å­˜å’Œå®‰å…¨æ£€æŸ¥ï¼‰"""
    
        # === 1. æ‡’åŠ è½½åµŒå…¥æ¨¡å‹ ===
        if self.embedding_model is None:
            logger.info("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
            self.embedding_model = SentenceTransformer(
                'paraphrase-multilingual-MiniLM-L12-v2',
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            logger.info("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            logger.info("å¤ç”¨å·²åŠ è½½çš„åµŒå…¥æ¨¡å‹")

        # === 2. å®‰å…¨æ£€æŸ¥ï¼šæ˜¯å¦æœ‰æ–‡æœ¬å— ===
        if not self.chunks:
            logger.warning("âš ï¸ æ— å¯ç”¨äºæ„å»ºç´¢å¼•çš„æ–‡æœ¬å—ï¼Œè·³è¿‡ç´¢å¼•æ„å»º")
            self.index = None
            return

        # === 3. ç”Ÿæˆå‘é‡ ===
        logger.info(f"æ­£åœ¨ä¸º {len(self.chunks)} ä¸ªæ–‡æœ¬å—ç”Ÿæˆå‘é‡...")
        embeddings = self.embedding_model.encode(
            self.chunks,
            show_progress_bar=True,
            convert_to_numpy=True  # æ˜¾å¼æŒ‡å®šè¿”å› numpy array
        ).astype('float32')

        # === 4. æ„å»º FAISS ç´¢å¼• ===
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings)

        logger.info(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆ | ç»´åº¦: {dimension} | æ–‡æœ¬å—æ•°é‡: {len(self.chunks)}")
    
    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        """æ£€ç´¢æœ€ç›¸å…³çš„ k ä¸ªæ–‡æœ¬å—"""
        if self.index is None:
            raise RuntimeError("å‘é‡ç´¢å¼•æœªåˆå§‹åŒ–")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.encode([query])
        query_vector = np.array(query_vector).astype('float32')
        
        # æ£€ç´¢
        distances, indices = self.index.search(query_vector, k)
        
        # æ„å»ºç»“æœ
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.chunks):
                results.append({
                    "content": self.chunks[idx],
                    "metadata": self.chunk_metadatas[idx],
                    "score": float(distances[0][i])
                })
        
        return results
    
    def generate_answer(self, question: str, context_list: List[str]) -> str:
        """ä½¿ç”¨ Qwen ç”Ÿæˆç­”æ¡ˆ"""
        # æ‹¼æ¥ä¸Šä¸‹æ–‡ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        context = "\n\n".join(context_list[:2])  # åªç”¨å‰2ä¸ªæœ€ç›¸å…³
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç­”æ¡ˆï¼Œè¯·å›ç­”â€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®šâ€ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·ç›´æ¥ç»™å‡ºç®€æ´ç­”æ¡ˆï¼Œä¸è¦è§£é‡Šè¿‡ç¨‹ã€‚"""
        
        return call_qwen(prompt, model="qwen-max")
    
    def ask(self, question: str, top_k: int = 3) -> Dict[str, Any]:
        """ç«¯åˆ°ç«¯é—®ç­”"""
        logger.info(f"ğŸ” æ£€ç´¢ä¸­: '{question}'")
        retrieved = self.retrieve(question, k=top_k)
        
        contexts = [item["content"] for item in retrieved]
        answer = self.generate_answer(question, contexts)
        
        return {
            "question": question,
            "answer": answer,
            "retrieved_chunks": retrieved
        }


# ======================
# ä½¿ç”¨ç¤ºä¾‹
# ======================
if __name__ == "__main__":
    # é…ç½®è·¯å¾„ï¼ˆæ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
    DOC_PATH = "data/processed/Docker.json"
    
    # åˆå§‹åŒ– RAG ç³»ç»Ÿ
    rag = RAGSystem(DOC_PATH)
    
    # æµ‹è¯•é—®é¢˜
    questions = [
        "Dockeré•œåƒå¦‚ä½•æ„å»ºï¼Ÿ",
        "å®¹å™¨å’Œé•œåƒçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•æŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨ï¼Ÿ"
    ]
    
    for q in questions:
        print("\n" + "="*60)
        result = rag.ask(q)
        print(f"â“ é—®é¢˜: {result['question']}")
        print(f"âœ… ç­”æ¡ˆ: {result['answer']}")
        
        # æ‰“å°æ¥æºï¼ˆè°ƒè¯•ç”¨ï¼‰
        print("\nğŸ“š æ£€ç´¢åˆ°çš„ç‰‡æ®µ:")
        for i, chunk in enumerate(result["retrieved_chunks"][:2]):
            source = chunk["metadata"]["source"]
            preview = chunk["content"][:100].replace('\n', ' ')
            print(f"  [{i+1}] ({source}) {preview}...")