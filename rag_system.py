# rag_system.py
import os
import json
import logging
import torch
import re
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# å¯¼å…¥ä½ å·²æœ‰çš„ Qwen å®¢æˆ·ç«¯
from qwen_client import call_qwen

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGSystem:
    def __init__(self, document_path: str, chunk_size: int = 500):
        """
        åˆå§‹åŒ– RAG ç³»ç»Ÿï¼ˆä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥æ–‡æœ¬åˆ†å—ï¼‰
        
        Args:
            document_path (str): JSON æ–‡æ¡£è·¯å¾„ï¼ˆæ¥è‡ª pdf_to_json.pyï¼‰
            chunk_size (int): æ¯ä¸ªæ–‡æœ¬å—çš„ç›®æ ‡æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 500ï¼‰
        """
        self.document_path = Path(document_path)
        self.chunk_size = chunk_size
        self.chunks = []          # List[str]
        self.chunk_metadatas = [] # List[Dict]
        self.index = None         # FAISS index
        self.embedding_model = None
        
        self._load_documents()
        self._chunk_documents()
        self._build_vector_index()
    
    def _load_documents(self):
        """ä» JSON åŠ è½½æ–‡æ¡£å†…å®¹"""
        if not self.document_path.exists():
            raise FileNotFoundError(f"æ–‡æ¡£ä¸å­˜åœ¨: {self.document_path}")
        
        with open(self.document_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1. æ¥è‡ª pdf_to_json.py çš„ {"pages": [...]}
        # 2. ç®€å•åˆ—è¡¨ ["text1", "text2"]
        if "pages" in data:
            texts = [page["content"] for page in data["pages"]]
            sources = [f"{data['metadata']['source_file']}:p{page['page_number']}" 
                      for page in data["pages"]]
        else:
            texts = data if isinstance(data, list) else [data.get("text", "")]
            sources = ["unknown"] * len(texts)
        
        self.raw_texts = texts
        self.sources = sources
        logger.info(f"âœ… åŠ è½½ {len(texts)} æ®µåŸå§‹æ–‡æœ¬")

    def _split_into_sentences(self, text: str) -> List[str]:
        """æŒ‰ä¸­è‹±æ–‡å¥æœ«æ ‡ç‚¹åˆ‡åˆ†å¥å­"""
        sentence_endings = r'[ã€‚ï¼ï¼Ÿ\.!?]'
        parts = re.split(f'({sentence_endings})', text)
        sentences = []
        i = 0
        while i < len(parts):
            if i + 1 < len(parts) and re.match(sentence_endings, parts[i + 1]):
                sentences.append(parts[i] + parts[i + 1])
                i += 2
            else:
                if parts[i].strip():
                    sentences.append(parts[i])
                i += 1
        return [s.strip() for s in sentences if s.strip()]

    def _chunk_documents(self):
        """å°†é•¿æ–‡æœ¬æŒ‰è¯­ä¹‰è¾¹ç•Œï¼ˆæ®µè½/å¥å­ï¼‰åˆ‡åˆ†ä¸ºå°å—"""
        self.chunks = []
        self.chunk_metadatas = []

        for i, text in enumerate(self.raw_texts):
            if not text.strip():
                continue

            source = self.sources[i]
            current_chunk = ""
            
            # æŒ‰è‡ªç„¶æ®µè½åˆ†å‰²ï¼ˆåŒæ¢è¡Œç¬¦ï¼‰
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            
            for para in paragraphs:
                if len(para) <= self.chunk_size:
                    # çŸ­æ®µè½ï¼šå°è¯•åˆå¹¶åˆ°å½“å‰å—
                    if current_chunk and len(current_chunk) + len(para) + 2 <= self.chunk_size:
                        current_chunk += "\n\n" + para
                    else:
                        # ä¿å­˜å½“å‰å—
                        if current_chunk:
                            self.chunks.append(current_chunk)
                            self.chunk_metadatas.append({
                                "source": source,
                                "chunk_id": len(self.chunks) - 1,
                                "start_char": -1,
                                "end_char": -1
                            })
                        current_chunk = para
                else:
                    # é•¿æ®µè½ï¼šæŒ‰å¥å­åˆ‡åˆ†
                    sentences = self._split_into_sentences(para)
                    for sent in sentences:
                        if current_chunk and len(current_chunk) + len(sent) + 1 <= self.chunk_size:
                            current_chunk += " " + sent
                        else:
                            if current_chunk:
                                self.chunks.append(current_chunk)
                                self.chunk_metadatas.append({
                                    "source": source,
                                    "chunk_id": len(self.chunks) - 1,
                                    "start_char": -1,
                                    "end_char": -1
                                })
                            current_chunk = sent
            
            # æ·»åŠ æœ€åä¸€å—
            if current_chunk:
                self.chunks.append(current_chunk)
                self.chunk_metadatas.append({
                    "source": source,
                    "chunk_id": len(self.chunks) - 1,
                    "start_char": -1,
                    "end_char": -1
                })

        logger.info(f"âœ… åˆ‡åˆ†ä¸º {len(self.chunks)} ä¸ªè¯­ä¹‰æ–‡æœ¬å—")

    def _build_vector_index(self):
        """æ„å»º FAISS å‘é‡ç´¢å¼•ï¼ˆå¸¦ç¼“å­˜å’Œå®‰å…¨æ£€æŸ¥ï¼‰"""
        if self.embedding_model is None:
            logger.info("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.embedding_model = SentenceTransformer(
                'paraphrase-multilingual-MiniLM-L12-v2',
                device=device
            )
            logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè®¾å¤‡: {device}ï¼‰")
        else:
            logger.info("å¤ç”¨å·²åŠ è½½çš„åµŒå…¥æ¨¡å‹")

        if not self.chunks:
            logger.warning("âš ï¸ æ— å¯ç”¨äºæ„å»ºç´¢å¼•çš„æ–‡æœ¬å—ï¼Œè·³è¿‡ç´¢å¼•æ„å»º")
            self.index = None
            return

        logger.info(f"æ­£åœ¨ä¸º {len(self.chunks)} ä¸ªæ–‡æœ¬å—ç”Ÿæˆå‘é‡...")
        embeddings = self.embedding_model.encode(
            self.chunks,
            show_progress_bar=True,
            convert_to_numpy=True
        ).astype('float32')

        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings)

        logger.info(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆ | ç»´åº¦: {dimension} | æ–‡æœ¬å—æ•°é‡: {len(self.chunks)}")
    
    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        """æ£€ç´¢æœ€ç›¸å…³çš„ k ä¸ªæ–‡æœ¬å—"""
        if self.index is None:
            raise RuntimeError("å‘é‡ç´¢å¼•æœªåˆå§‹åŒ–")
        
        query_vector = self.embedding_model.encode([query])
        query_vector = np.array(query_vector).astype('float32')
        distances, indices = self.index.search(query_vector, k)
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.chunks):
                results.append({
                    "content": self.chunks[idx],
                    "metadata": self.chunk_metadatas[idx],
                    "score": float(distances[0][i])
                })
        return results
    
    def generate_answer(self, question: str, context_list: List[str], max_contexts: int = 3) -> str:
        """ä½¿ç”¨ Qwen ç”Ÿæˆç­”æ¡ˆï¼ˆåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼‰"""
        if not context_list:
            return "æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®š"

        selected_contexts = []
        total_len = 0
        max_chars = 2000  # ä¿å®ˆé™åˆ¶ï¼Œé¿å…è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡

        for ctx in context_list[:max_contexts]:
            if total_len + len(ctx) > max_chars:
                break
            selected_contexts.append(ctx)
            total_len += len(ctx)

        context = "\n\n".join(selected_contexts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ AI åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ã€ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ã€‚
- å¦‚æœä¸Šä¸‹æ–‡åŒ…å«è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·ç›´æ¥ç»™å‡º**ç®€æ´ã€å‡†ç¡®**çš„ç­”æ¡ˆã€‚
- å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®šâ€ã€‚
- ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œä¸è¦è§£é‡Šæ¨ç†è¿‡ç¨‹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š
{question}

ã€å›ç­”ã€‘
"""

        try:
            response = call_qwen(prompt, model="qwen-max")
            return response.strip()
        except Exception as e:
            logger.error(f"Qwen è°ƒç”¨å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"
    
    def ask(self, question: str, top_k: int = 3) -> Dict[str, Any]:
        """ç«¯åˆ°ç«¯é—®ç­”"""
        logger.info(f"ğŸ” æ£€ç´¢ä¸­: '{question}'")
        retrieved = self.retrieve(question, k=top_k)
        contexts = [item["content"] for item in retrieved]
        answer = self.generate_answer(question, contexts)
        
        return {
            "question": question,
            "answer": answer,
            "retrieved_chunks": retrieved
        }


# ======================
# ä½¿ç”¨ç¤ºä¾‹
# ======================
if __name__ == "__main__":
    DOC_PATH = "data/processed/Docker.json"
    
    rag = RAGSystem(DOC_PATH, chunk_size=500)
    
    questions = [
        "Dockeré•œåƒå¦‚ä½•æ„å»ºï¼Ÿ",
        "å®¹å™¨å’Œé•œåƒçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•æŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨ï¼Ÿ"
    ]
    
    for q in questions:
        print("\n" + "="*60)
        result = rag.ask(q)
        print(f"â“ é—®é¢˜: {result['question']}")
        print(f"âœ… ç­”æ¡ˆ: {result['answer']}")
        
        print("\nğŸ“š æ£€ç´¢åˆ°çš„ç‰‡æ®µ:")
        for i, chunk in enumerate(result["retrieved_chunks"][:2]):
            source = chunk["metadata"]["source"]
            preview = chunk["content"][:100].replace('\n', ' ')
            print(f"  [{i+1}] ({source}) {preview}...")